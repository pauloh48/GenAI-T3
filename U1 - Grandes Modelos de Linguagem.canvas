{
	"nodes":[
		{"id":"fb7b3eb38dbb1db6","type":"text","text":"# U1. Grandes Modelos de Linguagem (1)","x":-691,"y":-65,"width":351,"height":85},
		{"id":"bc2ea0de7fa4ab05","type":"file","file":"GenAI T3/M9 Arquiteturas neurais avançadas para IA generativa para texto/assets/U1/mapa_mental.png","x":-200,"y":160,"width":729,"height":387},
		{"id":"3628047580050830","type":"text","text":"* Primeiras formas de modelos de linguagem, baseados em estatísticas.\n- **Como funcionam:** usam **probabilidades** para prever a próxima palavra em uma sequência.\n\t- **Exemplo 1 – N-gramas:** olham para as últimas N-1 palavras para prever a próxima (ex: “boa” → sugere “tarde”).\n\t- **Exemplo 2 – HMM (Modelos Ocultos de Markov):** usam **probabilidades de transição** entre estados (palavras ou sons).","x":1086,"y":-200,"width":962,"height":148},
		{"id":"0568cb4ac078f792","type":"text","text":"- **Vantagens:**\n    - Simples de implementar.\n    - Computacionalmente baratos.\n        \n- **Limitações:**\n    - Têm **“memória curta”** → só olham para poucas palavras anteriores.\n    - Não entendem bem frases longas nem o **significado profundo** das palavras.","x":1086,"y":-20,"width":962,"height":180},
		{"id":"d239abc7ef117f7e","type":"text","text":"### Modelos de Linguagem Probabilísticos","x":660,"y":-62,"width":290,"height":82},
		{"id":"51f40e640925de99","type":"text","text":"### Modelos de Linguagem Neurais (MLN)","x":660,"y":354,"width":290,"height":80},
		{"id":"139ccb65cfc9b3cd","type":"text","text":"- Evolução dos modelos probabilísticos, usam redes neurais para processar linguagem.\n- **Como funcionam:** transformam palavras em **vetores numéricos** (representações vetoriais) que capturam **significado e contexto**.\n\n- **Exemplos:**\n    - **RNN (Redes Neurais Recorrentes):** leem frases palavra por palavra, mantendo um “estado” do que já foi entendido.\n    - **LSTM (Long Short-Term Memory):** versão avançada das RNNs, conseguem lidar melhor com contextos longos.","x":1086,"y":208,"width":962,"height":186},
		{"id":"df4a1cbee7bb233b","type":"text","text":"- **Vantagens:**\n    - Capturam **contextos mais extensos**.\n    - Conseguem representar **significado semântico** das palavras.\n        \n- **Limitações:**\n    - **Treinamento pesado**, exige muito poder computacional.\n    - São **difíceis de interpretar** (caixa-preta).","x":1086,"y":434,"width":962,"height":173},
		{"id":"9e7103478fe4183f","type":"text","text":"## Modelos de Linguagem","x":-240,"y":-498,"width":326,"height":79},
		{"id":"5a7fc8095f9d778b","type":"text","text":"### **Principais usos:** \n","x":0,"y":-383,"width":220,"height":60},
		{"id":"decac36250ef9b93","type":"text","text":"- **O que são LMs?:** modelos que atribuem **probabilidades** às possíveis **próximas palavras** (ou à sequência inteira).\n- **Ideia central:** dado um trecho de texto, o LM calcula **quais palavras são mais prováveis** de vir em seguida.\n- **Treinamento autossupervisionado:** o modelo **aprende só prevendo a próxima palavra**; o próprio texto serve de “resposta correta” (não precisa de rótulos humanos).\n- **Por que isso funciona?** Ao tentar prever a próxima palavra, o LM **absorve padrões da língua** (gramática, ritmo, significados, relações entre termos).\n- **Intuição simples:** quanto mais o modelo lê, melhor ele fica em “**adivinhar com base no contexto**”, e isso já é suficiente para aprender muito sobre linguagem.\n","x":260,"y":-860,"width":910,"height":220},
		{"id":"c1ae029cdc748d8e","type":"text","text":"![[uso_1.png]]","x":360,"y":-595,"width":269,"height":272},
		{"id":"f133d439f24ea815","type":"text","text":"### Modelos de Linguagem Pré-treinados","x":660,"y":960,"width":290,"height":80},
		{"id":"33b4397e5fb9cee3","type":"text","text":"- São modelos neurais que passam por uma etapa inicial de treinamento em enormes volumes de dados textuais, adquirindo **conhecimento geral da linguagem**. Depois, podem ser ajustados para tarefas específicas com poucos dados adicionais (**transfer learning**).\n- **Como funcionam:** primeiro aprendem com textos de larga escala (livros, artigos, sites), captando padrões e nuances da linguagem. Depois, são adaptados para tarefas específicas (ex.: tradução, busca, análise de sentimentos).\n    \n- **Exemplos:**\n    - **BERT:** “lê” frases inteiras de uma vez, entendendo o contexto de cada palavra em relação às outras. Muito usado em sistemas de busca.\n    - **ELMo:** cria representações de palavras que mudam conforme o contexto em que aparecem.","x":1086,"y":740,"width":962,"height":260},
		{"id":"945f4f1501b78a86","type":"text","text":"**Vantagens:**\n    - Compreendem **contextos complexos** e relações sutis entre palavras.\n    -  **Alto desempenho** em tarefas específicas mesmo com poucos dados extras.\n        \n- **Limitações:**\n    - **Complexos** de treinar e ajustar.\n    - Exigem **muito poder computacional**, tanto no pré-treinamento quanto no uso em produção.","x":1086,"y":1040,"width":962,"height":200},
		{"id":"41e0615c7f99db95","type":"text","text":"### Grandes Modelos de Linguagem (LLMs)","x":660,"y":1380,"width":290,"height":80},
		{"id":"374628eae111be73","type":"text","text":"- Evolução dos modelos pré-treinados, construídos com arquiteturas **massivas** que podem ter bilhões ou até trilhões de parâmetros.\n- **Como funcionam:** devido à sua escala enorme, conseguem **compreender linguagem, contexto e nuances** com muito mais profundidade. São treinados em bases de dados gigantescas e aplicam técnicas avançadas (como Transformers) para processar e gerar texto.\n\n- **Exemplos:**\n- **GPT-4.5® (OpenAI®)**; **Claude 3® / Claude 3.7 Sonnet® (Anthropic®)**; **Gemini 2.5 Pró® (Google®)**; **Llama® (Meta®)**","x":1086,"y":1280,"width":962,"height":200},
		{"id":"ea6c428f09925fce","type":"text","text":"- **Vantagens:**\n    \n    - **Versatilidade**: conseguem executar uma ampla variedade de tarefas (tradução, redação, programação, resumo, análise etc.).\n    - **Alta qualidade** na geração de textos, com coerência e riqueza de detalhes.\n    - **Compreensão profunda** de contexto e comunicação humana.\n        \n- **Limitações:**\n    - Exigem **enormes recursos computacionais** para treino e uso.\n    - Podem apresentar **viés** ou gerar informações incorretas (“alucinações”).\n    - Alto **custo de desenvolvimento e manutenção**.","x":1086,"y":1520,"width":962,"height":260},
		{"id":"81396fbef5b6d2e4","type":"text","text":"![[evolucao.png]]","x":-195,"y":620,"width":720,"height":340,"color":"4"},
		{"id":"eec2046077ecda08","type":"text","text":"![[uso_3.png]]","x":651,"y":-595,"width":283,"height":272},
		{"id":"4ecea849e70247f5","type":"text","text":"![[uso_2.png]]","x":950,"y":-594,"width":250,"height":272},
		{"id":"6d831a801977d211","type":"text","text":"![[uso_4.png]]","x":1224,"y":-593,"width":250,"height":271},
		{"id":"36780efed2c38d2d","type":"text","text":"![[uso_5.png]]","x":1489,"y":-593,"width":250,"height":272},
		{"id":"9ecf6471d6862023","type":"text","text":"- **Assistentes virtuais:** Siri®, Alexa® e Google Assistente® utilizam LMs para entender comandos de voz e responder às perguntas dos/as usuários/as;\n- **Tradutores automáticos:** plataformas, como o Google Tradutor®, usam LMs para traduzir textos de forma cada vez mais precisa e natural;\n- **Preenchimento automático e sugestões de texto:** ao escrever um _email_ ou mensagem, os LMs predizem a próxima palavra, corrigem erros ortográficos e até sugerem respostas completas;\n- **Geração de conteúdo:** _blogs_ e _sites_ podem ser, em parte, escritos por LMs, que criam textos criativos e informativos;\n- **Análise de sentimentos:** empresas utilizam LMs para analisarem comentários e menções nas redes sociais, identificando a opinião do público sobre seus produtos e serviços.","x":1840,"y":-591,"width":910,"height":270},
		{"id":"a9b9d4831f6c6a96","type":"text","text":"https://platform.openai.com/tokenizer?ref=haihai.ai\n![[token_tx_1.png]]\n![[token_id_1.png]]","x":6191,"y":-696,"width":473,"height":580},
		{"id":"23800d85de79fc00","type":"text","text":"* https://lunary.ai/openai-tokenizer*\n![[token_tx_2.png]]\n![[token_id_2.png]]","x":6711,"y":-696,"width":520,"height":580},
		{"id":"36768f9e37437070","type":"text","text":"- **Facilitar o suporte a diferentes idiomas**\n    \n    - Algumas línguas, como o alemão e o finlandês, possuem palavras compostas enormes.\n        \n    - A tokenização quebra essas palavras em pedaços menores, tornando o processamento mais viável.\n        \n- **Lidar com palavras novas**\n    \n    - O vocabulário humano cresce sempre (novos termos e gírias).\n        \n    - Se o modelo não conhece uma palavra, pode dividi-la em partes conhecidas.\n        \n    - Exemplo: _criptomoeda_ → _cripto_ + _moeda_.","x":5191,"y":-476,"width":778,"height":220},
		{"id":"0cc34c22e95d6033","type":"text","text":"**Eficiência computacional**\n\n- Trabalhar com símbolos menores é mais leve que processar frases inteiras.\n    \n- A tokenização reduz a **complexidade computacional**, permitindo treinar e rodar modelos mais rápido e com menos recursos.","x":5191,"y":-236,"width":778,"height":137},
		{"id":"37e06b31a8831aa6","type":"text","text":"![[ex_geraca.png]]","x":6071,"y":-83,"width":480,"height":374},
		{"id":"a4180979efaaac2d","type":"text","text":"**Exemplo prático:**\n\n- Frase: “Os gatos brincam felizes com …”\n- O Transformer usa o contexto inteiro da frase para prever a próxima palavra (ex.: “bola”).\n- Cada nova palavra gerada também entra no contexto para as próximas previsões.","x":5580,"y":-16,"width":389,"height":240},
		{"id":"e3959e78e56b4103","type":"text","text":"Esse processo de escolher a próxima palavra é chamado de **decodificação**","x":5511,"y":409,"width":580,"height":50},
		{"id":"871c17743541bc89","type":"text","text":"* **O que acontece na geração de texto**\n\t- O modelo gera **uma palavra por vez (token)**, de forma **autoregressiva** → cada nova palavra depende das anteriores.\n\t- Ele calcula uma **probabilidade para cada palavra possível**.\n\t    ","x":5131,"y":527,"width":569,"height":117},
		{"id":"e56568966026ca84","type":"text","text":"- “Os gatos brincam felizes com a ___”\n\t- \"bola\" = 70%\n\t- \"zebra\" = 10%\n\t- Mais provável → \"bola\".","x":5798,"y":527,"width":428,"height":110},
		{"id":"ee3a7fe9dfc25c9b","type":"text","text":"- **Vantagem:** textos mais **diversos e criativos**, evitando repetições.\n- **Desafio:** risco de gerar palavras incoerentes ou fora do contexto se a aleatoriedade for muito alta.","x":5801,"y":727,"width":530,"height":110},
		{"id":"2d4f3132f0111986","type":"text","text":"#### Geração de Texto por Amostragem em Arquiteturas _Transformer_","x":4589,"y":644,"width":451,"height":91},
		{"id":"db67b7757e2ed59b","type":"text","text":"* **Amostragem**\n\t- **Definição:** processo de escolher a próxima palavra com base em probabilidades.\n\t- Diferente da **decodificação gulosa** (que sempre escolhe a mais provável), a amostragem **introduz aleatoriedade controlada**, permitindo que palavras menos prováveis apareçam.","x":5133,"y":690,"width":567,"height":183},
		{"id":"fa472de1922e4f26","type":"text","text":"- **Top-k sampling:**\n    \n    - Limita a escolha às **k palavras mais prováveis**.\n        \n    - Dá variedade, mas com controle.","x":5325,"y":984,"width":510,"height":100},
		{"id":"9f5df59f8deb1ccc","type":"text","text":"### Técnicas para equilibrar\n\n\n        \n","x":4910,"y":1114,"width":295,"height":58},
		{"id":"32733aa360e48970","type":"text","text":"* **Como funciona**\n\t- O modelo ordena todas as palavras possíveis por **probabilidade**.\n\t- Mantém apenas as **k palavras mais prováveis**.\n\t- A próxima palavra é escolhida **aleatoriamente** dentro desse conjunto reduzido.","x":5911,"y":888,"width":669,"height":126},
		{"id":"29d248a0c860e340","type":"text","text":"* Vantagens\n\t- Garante **coerência**, pois só considera palavras plausíveis.\n\t- Introduz **variedade**, evitando repetições excessivas.","x":5911,"y":1106,"width":546,"height":98},
		{"id":"e7fc24f40f5b3fba","type":"text","text":"* Limitação: O valor de **k precisa ser ajustado**:","x":6531,"y":1126,"width":409,"height":58},
		{"id":"f8ce18f8df22c60a","type":"text","text":"![[topk.png]]","x":6712,"y":743,"width":457,"height":300,"color":"#000000"},
		{"id":"ced2b5c38ef92c04","type":"text","text":"* Vantagens\n\t- É **dinâmico**: o tamanho do conjunto de palavras varia de acordo com a distribuição de probabilidades.\n\t- Permite **flexibilidade**: considera poucas palavras em contextos claros e mais palavras em contextos ambíguos.\n\t- Gera textos **variados**, mantendo a coerência.","x":5950,"y":1737,"width":741,"height":203},
		{"id":"08fbca0f8b3a5cc1","type":"text","text":"* Como funciona\n\t* Define-se um limiar de probabilidade cumulativa p (ex.: 0.9).\n\t* O modelo seleciona o menor conjunto de palavras cuja soma das probabilidades atinge ou ultrapassa p.\n\t* Esse conjunto é chamado de “núcleo”.\n\t* A próxima palavra é escolhida aleatoriamente dentro do núcleo, respeitando as probabilidades originais.","x":5950,"y":1442,"width":741,"height":205},
		{"id":"3874d43e308ad199","type":"text","text":"- **Top-p (nucleus) sampling:**\n    \n    - Escolhe dentro do menor conjunto de palavras cuja soma de probabilidades atinge **p** (ex.: 0,9 ou 90%).\n        \n    - Mais dinâmico que o top-k, pois se adapta ao contexto.","x":5325,"y":1544,"width":510,"height":130},
		{"id":"ec2a7824a99ae77f","type":"text","text":" **O que é**\n- Hiperparâmetro que **controla a aleatoriedade** na geração de texto.\n- Ajusta a **distribuição de probabilidades** dos tokens previstos pelo modelo.\n- Funciona como um \"botão\" que equilibra **precisão** e **criatividade**.","x":4972,"y":1868,"width":608,"height":136},
		{"id":"61a79ae1cf2e4c08","type":"text","text":"* Temperatura Alta (≥ 1,0 — ex.: 1,5 ou 2,0)\n\t- **Probabilidades ficam mais uniformes → modelo considera palavras menos prováveis.**\n\t- **Texto gerado: criativo, exploratório, surpreendente.**\n\t- **Riscos: menor coerência, chance de alucinações e incoerências.**\n\t- **Ideal para: poemas, roteiros, histórias criativas.**","x":5191,"y":2023,"width":618,"height":168},
		{"id":"1848302e29efcffe","type":"text","text":"### Temperatura","x":4691,"y":2161,"width":250,"height":60},
		{"id":"655b2e3dd913072e","type":"text","text":"![[temp_alta.png]]","x":5895,"y":2004,"width":196,"height":187},
		{"id":"7b622f64f0d9dae3","type":"text","text":"* **Temperatura Baixa (≤ 0,5 — ex.: 0,2 ou 0,5)**\n\t- Diferenças entre probabilidades ficam **amplificadas**.\n\t- Modelo tende a escolher sempre as palavras mais prováveis.\n\t- Texto gerado: **mais previsível, consistente, preciso**.\n\t- Riscos: pode ficar **repetitivo** ou preso em loops.\n\t- Ideal para: **resumos, traduções, respostas precisas**.","x":5191,"y":2199,"width":618,"height":180},
		{"id":"1187b72eed9b3ca9","type":"text","text":"Ao usar com top-p, a temperatura ajusta a distribuição e com top-p se escolhe os candidatos","x":5645,"y":1875,"width":250,"height":122},
		{"id":"43688a7c471368bc","type":"text","text":"![[temp_baixa.png]]","x":6125,"y":2098,"width":196,"height":187},
		{"id":"ba98656cc5d40925","type":"text","text":"![[temp_0.png]]","x":6372,"y":2201,"width":196,"height":191},
		{"id":"4063e89cbf7fa2c2","type":"text","text":"**Temperatura > 1 (ex.: 10):**\n- Diferenças suavizadas → distribuição mais uniforme entre palavras.\n    \n- Palavras improváveis (_omelete, amor_) ganham mais chance.\n    \n- Resultado: texto mais criativo, mas menos consistente.","x":6016,"y":2824,"width":713,"height":140},
		{"id":"c02c2051555cfdc6","type":"text","text":"![[temsp.png]]","x":6016,"y":2984,"width":713,"height":200},
		{"id":"80f1f24b115064bc","type":"text","text":"- **Logits**: saídas não normalizadas (! soma 1)do modelo antes da conversão em probabilidades.\n- **Softmax**: transforma logits em probabilidades usadas para escolher a próxima palavra.","x":6020,"y":2432,"width":704,"height":104},
		{"id":"acbe584dae339470","type":"text","text":"**Temperatura = 1 (padrão):**\n- Probabilidades seguem os logits originais.\n- Exemplo: _filme_ (0,54), _documentário_ (0,36).","x":6020,"y":2564,"width":704,"height":110},
		{"id":"c8b6f4b70777c484","type":"text","text":"**Temperatura < 1 (ex.: 0,3):**\n- Diferenças ampliadas → probabilidade mais concentrada na palavra mais provável.\n- Exemplo: _filme_ sobe para 0,79.\n- Resultado: texto mais previsível e menos criativo.","x":6020,"y":2684,"width":704,"height":133},
		{"id":"aaf8d853c9a80b4d","type":"text","text":"* Temperatura = 0\n\t- Sempre escolhe o **token mais provável**.\n\t- Resultado: **altamente determinístico**, sem criatividade.","x":5191,"y":2392,"width":618,"height":92},
		{"id":"eec16f8adabcd2ef","type":"text","text":"![[teperatura.png]]","x":5191,"y":2594,"width":618,"height":266},
		{"id":"1004546655be250d","type":"text","text":"* Limitação\n\t- A escolha do **valor de p** é sensível:\n\t    - Muito baixo → restringe demais e pode perder diversidade.\n\t    - Muito alto → pode incluir palavras incoerentes.\n\t- Um valor típico de p utilizado é 0,90, o que permite que o modelo mantenha uma boa combinação de coerência e criatividade","x":6749,"y":1753,"width":840,"height":171},
		{"id":"767b64bfb5c23a20","type":"text","text":"![[topp.png]]","x":6791,"y":1368,"width":431,"height":353},
		{"id":"383b1d4e133056b3","type":"text","text":"- Muito baixo → texto previsível e repetitivo.\n\t- Usado em tarefas que exigem **precisão e consistência**.\n\t- Exemplos: tradução automática, geração de respostas em assistentes virtuais.\n\t  ","x":7091,"y":1082,"width":539,"height":122},
		{"id":"4d0ef584a71d7db1","type":"text","text":"  - Muito alto → pode perder coerência (alucinação).\n\t\t    - Usado em tarefas que valorizam **originalidade e criatividade**.\n\t\t\t- Exemplos: geração de histórias, criação de poesia.","x":7091,"y":1233,"width":539,"height":100},
		{"id":"e77d7f293b5eec5e","type":"text","text":"- Exemplo de **top-k com k=2** aplicado na frase: _“Ontem fui ao cinema ver um …”_.\n    \n- O modelo calcula probabilidades para várias palavras (ex.: _filme, documentário, omelete_).\n    \n- Apenas as **duas palavras mais prováveis** são mantidas: _“filme”_ e _“documentário”_.\n    \n- As demais opções são **descartadas**.\n    \n- As probabilidades são **renormalizadas** (para a soma ser 1, 100%), resultando em:\n    \n    - _“filme”_ = 0,59\n        \n    - _“documentário”_ = 0,41\n        \n- O top-k **restringe a variedade**, focando nas opções mais prováveis.","x":7277,"y":772,"width":709,"height":242},
		{"id":"b153ab26a7ed737f","type":"text","text":"- Exemplo de **amostragem top-p** aplicado à frase: _“Ontem fui ao cinema ver um …”_.\n    \n- O modelo gera probabilidades para várias palavras (ex.: _filme, documentário, omelete, amor, gostar_).\n    \n- As probabilidades são **acumuladas em ordem decrescente** → formando uma distribuição cumulativa.\n    \n- Com **p = 0,95**, selecionam-se as palavras cujas probabilidades somadas atingem o limite (ex.: _filme_ + _documentário_).\n    \n- Palavras de baixa probabilidade (_omelete, amor, gostar_) são **descartadas**.\n    \n- As probabilidades das palavras escolhidas são **renormalizadas**:\n    \n    - _filme_ = 0,59\n        \n    - _documentário_ = 0,41\n        \n- O top-p mantém **diversidade controlada**, escolhendo apenas as opções mais relevantes dentro do contexto.","x":7277,"y":1368,"width":709,"height":353},
		{"id":"e49f538c54b0ffd9","type":"text","text":"### Importância da Tokenização em LLMs","x":4553,"y":-436,"width":431,"height":50},
		{"id":"258baab9321bd97a","type":"text","text":"* **Limitação dos modelos anteriores (RNNs, LSTMs):**\n\t- Líam  texto palavra por palavra, em ordem.\n\t- Dificuldade em “lembrar” informações distantes no texto.\n\t- Processamento lento, sem possibilidade de olhar para trás facilmente.","x":4704,"y":-16,"width":807,"height":131},
		{"id":"8b240aa476b1c1f9","type":"text","text":"### LLMs com Arquitetura  _Transformer_","x":4249,"y":82,"width":317,"height":66},
		{"id":"0e4a0621a6983cfb","type":"text","text":"* **Como o Transformer funciona:**\n\t- Consegue acessar **todo o contexto de uma vez**, em vez de só o mais recente.\n\t- Considera não apenas a frase até o ponto atual, mas também suas próprias palavras geradas antes.\n\t- Isso permite capturar relações entre palavras próximas **e** distantes em uma sequência longa.","x":4704,"y":124,"width":807,"height":120},
		{"id":"ef3350979e1ce992","type":"text","text":"**Vantagem principal:**\n\n- Gera textos mais **coerentes e contextualmente corretos**.\n    \n- Funciona como um escritor que **relê o parágrafo anterior** antes de continuar a escrever.","x":4704,"y":264,"width":689,"height":145},
		{"id":"c3b413fbc008f354","type":"text","text":"# U1. Grandes Modelos de Linguagem (2)","x":3040,"y":-83,"width":351,"height":85},
		{"id":"b4881964ab764cf4","type":"text","text":"- **O que é:** primeiro passo para que o modelo entenda o texto.\n    \n- **Como funciona:** o texto é dividido em partes menores chamadas **tokens**.\n    \n- **Tipos de tokens:**\n    \n    - Palavra inteira → _\"casa\"_\n        \n    - Subpalavra → _\"caminh\"_ + _\"ando\"_\n        \n    - Caractere → _\"a\"_, _\"b\"_, _\"c\"_\n        \n- Cada token recebe um **ID numérico** único, que o modelo usa como entrada.","x":4536,"y":-1147,"width":619,"height":211},
		{"id":"fe74750858ecfa37","type":"text","text":"| **Característica**         | **Tokenização**                                                                           | **Incorporação (Embedding)**                                                        |\n| -------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n| **Definição**              | Converte o texto em tokens individuais (palavras, subpalavras ou caracteres).             | Converte tokens em representações vetoriais densas.                                 |\n| **Propósito**              | Dividir o texto em unidades menores e gerenciáveis.                                       | Capturar o **significado semântico** dos tokens em forma numérica.                  |\n| **Saída**                  | Sequência de tokens (ex.: `[\"Processamento\", \"de\", \"linguagem\", \"natural\"]`).             | Sequência de vetores densos (ex.: `[vetor1, vetor2, vetor3, vetor4]`).              |\n| **Exemplo**                | \"cachorro\" → tokens: `[\"ca\", \"chorro\"]`                                                   | \"cachorro\" → embedding: `[0.12, -0.45, 0.88, ...]`                                  |\n| **Ferramentas**            | Tokenizadores: WordPiece, BPE (Byte Pair Encoding), SentencePiece.                        | Modelos/algoritmos: Word2Vec, GloVe, BERT, DeBERTa.                                 |\n| **Granularidade**          | Gera várias unidades (tokens), geralmente mais do que palavras reais.                     | Cada token vira um vetor em um espaço de menor dimensão.                            |\n| **Dependência de idioma**  | Depende da estrutura do idioma (ex.: palavras compostas em alemão, sufixos no português). | Mais independente da língua, mas os embeddings capturam nuances semânticas.         |\n| **Requisito de dados**     | Precisa de um **vocabulário** criado a partir dos dados de treinamento.                   | Precisa de **modelos pré-treinados** ou grandes dados para aprender representações. |\n| **Bibliotecas comuns**     | `nltk`, `spaCy`, `transformers`.                                                          | `gensim`, `torch.nn.Embedding`, `transformers`.                                     |\n| **Uso em pipeline de PNL** | Primeira etapa: dividir o texto bruto em tokens.                                          | Segunda etapa: transformar os tokens em vetores numéricos para entrada no modelo.   |\n","x":5251,"y":-1147,"width":1517,"height":402},
		{"id":"e325efe65cc6a0c5","type":"text","text":"#### Diferença entre Token, Vetor e Embedding","x":4536,"y":-846,"width":448,"height":50},
		{"id":"69d4746c85e10248","type":"text","text":"- **Uniformizar o processamento**\n    \n    - Palavras têm tamanhos e complexidades diferentes.\n        \n    - Quebrar em tokens cria uma forma mais padronizada de lidar com textos variados.\n        \n- **Aproveitar partes comuns das palavras**\n    \n    - Muitos idiomas usam **prefixos, sufixos e radicais**.\n        \n    - Exemplo: _inteligente_ e _inteligência_ compartilham “intelig”, economizando esforço do modelo.","x":5191,"y":-676,"width":778,"height":180},
		{"id":"be0ac761dd5178c6","type":"text","text":"Refere-se a LMs pré-treinados de grande porte","x":3846,"y":-996,"width":400,"height":50},
		{"id":"c221ce6e83f88a43","type":"text","text":"## Grandes Modelos de Linguagem","x":3611,"y":-836,"width":299,"height":76},
		{"id":"cb5fafbd55c722da","type":"text","text":"### _Tokenização_ em  LLMs","x":4071,"y":-896,"width":269,"height":50},
		{"id":"f40b31d2df6c2571","type":"text","text":"## Treinamento de Grandes Modelos de Linguagem","x":9064,"y":-285,"width":327,"height":98},
		{"id":"4fe51916843807ed","type":"text","text":"- **Função de perda**: o modelo ajusta seus parâmetros para reduzir o erro na previsão da próxima palavra, usando a **entropia cruzada**.\n    \n- **Objetivo**: aprender padrões da língua, como quais palavras aparecem juntas e em quais contextos, para conseguir **gerar texto coerente**.","x":9531,"y":-157,"width":804,"height":115},
		{"id":"e87bfbb4587a0a59","type":"text","text":"**Aprendizado auto-supervisionado**:\n\n- Não precisamos fornecer respostas manuais.\n    \n- O próprio texto serve como supervisão, pois a palavra seguinte já está na sequência original.","x":9531,"y":-287,"width":804,"height":118},
		{"id":"fbbdaa12685b0629","type":"text","text":"- **Entrada de dados**: usamos um _corpus_, ou seja, uma grande coleção de textos (livros, artigos, sites, etc.).\n- **Tarefa do modelo**: dado um pedaço de texto, o modelo deve prever qual será a próxima palavra.\n    - Exemplo: na frase “Os gatos brincam felizes com a …”, o modelo precisa escolher a palavra mais provável, como “bola” ou “lã”.","x":9531,"y":-420,"width":804,"height":115},
		{"id":"5ec1e06ce37e90b1","type":"text","text":"![[treinamento.png]]","x":9531,"y":35,"width":804,"height":465},
		{"id":"1221dffb61c0c48c","type":"text","text":"","x":9709,"y":77,"width":250,"height":60},
		{"id":"3ad942bbd8e16774","type":"text","text":"**Passo a passo do fluxo**\n1. **Embeddings de entrada**  \n    Cada palavra (“Os”, “gatos”, “brincam”, “felizes”, “com”) é transformada em um **vetor numérico** (embedding).  \n    ➝ Esses vetores representam o “significado” das palavras em um espaço matemático.\n2. **Bloco Transformer**  \n    Os embeddings passam por várias camadas do Transformer, que analisam como cada palavra se relaciona com as outras no contexto da frase.  \n    ➝ Ex.: o modelo aprende que “gatos” e “brincam” estão ligados.","x":10480,"y":-287,"width":826,"height":269},
		{"id":"b7e06b5861e4920a","type":"text","text":"\n        \n5. **Próxima Palavra (alvo correto)**  \n    A palavra verdadeira da frase de treinamento é usada como **resposta certa** (por exemplo, “a”).\n    \n6. **Cálculo da Perda (loss)**  \n    O modelo calcula o **erro** entre a probabilidade atribuída à palavra correta e o valor ideal.  \n    ➝ Isso é feito com a **entropia cruzada** (mostrada como `-log y_long`).","x":10480,"y":220,"width":826,"height":140},
		{"id":"e509286763d3e3b3","type":"text","text":"\n\n\n3. **Camada Linear**  \n    Depois do processamento, o modelo transforma essas representações internas em números correspondentes a todas as palavras do vocabulário.\n4. **Softmax**  (0 a 1 ou 0% a 100$ de probabilidade)\n    Essa camada converte os números em **probabilidades**.  \n    ➝ Para a posição após “com”, o modelo gera algo como:\n    - “bola”: 70%; “lã”: 20% “caixa”: 5%; outras palavras: valores menores.","x":10480,"y":0,"width":826,"height":200},
		{"id":"6220dafe72fcac03","type":"text","text":"\n    \n7. **Ajuste dos Pesos**  \n    Esse erro é usado para corrigir os parâmetros do modelo via **descida do gradiente**.  \n    ➝ Quanto mais o modelo erra, mais ele ajusta seus pesos.\n    \n8. **Média das perdas**  \n    O processo se repete para cada palavra da sequência, e a perda final é a **média de todas as perdas**.","x":10480,"y":380,"width":826,"height":140},
		{"id":"5afe45003c964137","type":"text","text":"* **Teacher forcing ocorre logo no início de cada passo** → na linha **“Embeddings de entrada”**.\n\t- Ou seja, cada posição da sequência é alimentada com a **palavra correta**, e não com o que o modelo “achou” que seria.\n\t- Isso garante que em cada estágio (camada Transformer, linear, softmax, cálculo da perda) o treinamento seja mais estável e rápido.","x":11400,"y":-226,"width":775,"height":147},
		{"id":"87372b99c8a1fc40","type":"text","text":"### Relação entre Tokens, Tamanho do Modelo e Custo de Treinamento","x":9301,"y":802,"width":408,"height":80},
		{"id":"0511fed77ad19207","type":"text","text":"- **Número de Tokens**\n    - São os pedaços de texto usados no treino.\n    - Mais tokens → modelo aprende mais padrões e nuances da língua.\n    - Porém → exige mais processamento, tempo e custo.\n        \n","x":9909,"y":626,"width":779,"height":126},
		{"id":"1045031e4b8420ca","type":"text","text":"- **Tamanho do Modelo (Parâmetros)**\n    - Parâmetros (são como “botões ajustáveis”), ou seja, w (peso) e b (bias)   é o que o modelo usa para prever palavras.\n    - Modelos maiores (mais parâmetros) → conseguem capturar mais detalhes da linguagem.\n    - Mas → precisam de muito mais memória, GPUs/TPUs potentes e, claro, mais dinheiro.\n","x":9909,"y":772,"width":779,"height":140},
		{"id":"78ecd9316886a10a","type":"text","text":"       \n- **Custo de Treinamento**\n    - Depende diretamente dos dois fatores: **tokens** + **parâmetros**.\n    - Mais tokens **e** mais parâmetros = custo altíssimo (milhões de dólares em energia e hardware).","x":9909,"y":932,"width":779,"height":100},
		{"id":"3d3621cd0785f49f","type":"text","text":"# U1. Grandes Modelos de Linguagem (3)","x":8440,"y":338,"width":351,"height":85},
		{"id":"5d9137eb9ad70526","type":"text","text":"- **Coleta da Web (web scraping)**\n    - Fonte mais comum: páginas da internet, redes sociais, notícias.\n    - Exemplo: **Common Crawl®**, que arquiva bilhões de páginas.\n    - Muitas vezes passam por **limpeza** → removendo spam, linguagem tóxica, código etc.\n    - Ex.: o **C4** (Colossal Clean Crawled Corpus) com 156 bilhões de tokens.\n        \n- **Dados existentes**\n    - Livros, artigos, Wikipédia®, patentes, jornais, transcrições.\n    - Ex.: o GPT-3® foi treinado em **429B tokens da web**, **67B de livros** e **3B da Wikipédia**.\n        ","x":9898,"y":1140,"width":790,"height":240},
		{"id":"8592b1c8548ea48d","type":"text","text":"\n- **Crowdsourcing (colaboração em massa)**\n    - Dados criados ou anotados por muitas pessoas online.\n    - Vantagem: rápido e barato.\n    - Risco: precisa de **controle de qualidade** para evitar erros.\n        \n- **Dados gerados automaticamente**\n    - Texto criado por outros LMs, imagens sintéticas, simulações.\n    - Úteis para **aumentar corpora existentes** ou treinar tarefas específicas.\n        \n","x":9898,"y":1400,"width":790,"height":220},
		{"id":"29dfe07860b0e1e8","type":"text","text":"- **Criação manual por especialistas**\n    - Produz dados de **alta qualidade** e precisão.\n    - Desvantagem: é **caro e demorado**, mas essencial em áreas que exigem conhecimento especializado.\n        \n- **Combinação de fontes**\n    - Misturar web, livros, dados manuais, crowdsourcing etc.\n    - Necessário cuidar da **compatibilidade e consistência**.","x":9898,"y":1640,"width":800,"height":200},
		{"id":"b26d734b4e922f53","type":"text","text":"### _Corpora_ de Treinamento para Grandes Modelos de Linguagem","x":9120,"y":1360,"width":481,"height":80},
		{"id":"96093af4e6f8c788","type":"text","text":"## Desafios das LLMs","x":9120,"y":2156,"width":285,"height":71},
		{"id":"9145f41d657a8e68","type":"text","text":"- **Alucinações (informações falsas)**\n    - LLMs geram texto coerente, mas não garantem que ele seja **verdadeiro**.\n    - Isso é arriscado em contextos onde os fatos são cruciais.\n        \n- **Linguagem tóxica e preconceitos**\n    - Mesmo com entradas neutras, os modelos podem gerar **discurso de ódio** ou **conteúdo ofensivo**.\n    - Também podem reforçar **estereótipos** e **atitudes negativas** contra grupos sociais.\n        ","x":9898,"y":1980,"width":790,"height":200},
		{"id":"5410bc67e4c8cc6f","type":"text","text":"\n- **Vieses nos dados**\n    - Dados da web incluem linguagem tóxica.\n    - Há concentração de textos de EUA e Europa → gera **visão de mundo limitada**.\n    - O modelo pode **amplificar preconceitos** existentes nos dados.\n        \n- **Uso malicioso**\n    - LLMs podem ser explorados para:\n        - **Desinformação**\n        - **Phishing** (fraudes por e-mail/mensagem)\n        - **Propaganda extremista** e **radicalização**\n            \n","x":9891,"y":2200,"width":797,"height":260},
		{"id":"647e132fec00ca08","type":"text","text":"- **Privacidade**\n    - Podem vazar informações sensíveis usadas no treinamento (ex.: nomes, endereços, prontuários médicos).\n        \n- **Alto custo e impacto ambiental**\n    - Treinar e rodar LLMs exige:\n        - **Muito poder computacional** (GPUs, supercomputadores).\n        - **Grande consumo de energia**.\n        - **Água para resfriamento** dos datacenters.\n            \n","x":9891,"y":2480,"width":797,"height":220},
		{"id":"9f77d67c4ab4da80","type":"text","text":"- **Mitigação e transparência**\n    - Pesquisas buscam formas de reduzir esses riscos.\n    - Importante:\n        - **Revisar e filtrar os dados de treino**.\n        - **Transparência sobre os corpora usados**.\n        - Regulamentações estão surgindo para exigir maior responsabilidade.","x":9891,"y":2720,"width":797,"height":180}
	],
	"edges":[
		{"id":"43b8bbfbfc35e7ae","fromNode":"fb7b3eb38dbb1db6","fromSide":"right","toNode":"9e7103478fe4183f","toSide":"left"},
		{"id":"8db2f69260f95cc6","fromNode":"9e7103478fe4183f","fromSide":"right","toNode":"decac36250ef9b93","toSide":"left"},
		{"id":"cdfff608ab9a8060","fromNode":"9e7103478fe4183f","fromSide":"bottom","toNode":"5a7fc8095f9d778b","toSide":"left"},
		{"id":"1875c23b6d896263","fromNode":"fb7b3eb38dbb1db6","fromSide":"right","toNode":"bc2ea0de7fa4ab05","toSide":"left"},
		{"id":"8f0a5352b4f5b996","fromNode":"bc2ea0de7fa4ab05","fromSide":"right","toNode":"d239abc7ef117f7e","toSide":"left"},
		{"id":"1491750087a1752f","fromNode":"bc2ea0de7fa4ab05","fromSide":"right","toNode":"51f40e640925de99","toSide":"left"},
		{"id":"dab956ac71a36e47","fromNode":"bc2ea0de7fa4ab05","fromSide":"right","toNode":"f133d439f24ea815","toSide":"left"},
		{"id":"5adedc9970c90400","fromNode":"bc2ea0de7fa4ab05","fromSide":"right","toNode":"41e0615c7f99db95","toSide":"left"},
		{"id":"03c7866affc9a7c8","fromNode":"d239abc7ef117f7e","fromSide":"right","toNode":"3628047580050830","toSide":"left"},
		{"id":"3866b3efcb427d87","fromNode":"d239abc7ef117f7e","fromSide":"right","toNode":"0568cb4ac078f792","toSide":"left"},
		{"id":"2e553166e55ebed6","fromNode":"51f40e640925de99","fromSide":"right","toNode":"139ccb65cfc9b3cd","toSide":"left"},
		{"id":"abcf860430f6d4e8","fromNode":"51f40e640925de99","fromSide":"right","toNode":"df4a1cbee7bb233b","toSide":"left"},
		{"id":"0fa10a1ce39c470e","fromNode":"5a7fc8095f9d778b","fromSide":"right","toNode":"c1ae029cdc748d8e","toSide":"left"},
		{"id":"0eeef13ab2e1656a","fromNode":"f133d439f24ea815","fromSide":"right","toNode":"33b4397e5fb9cee3","toSide":"left"},
		{"id":"a1e6bb318107eaaa","fromNode":"f133d439f24ea815","fromSide":"right","toNode":"945f4f1501b78a86","toSide":"left"},
		{"id":"09b01ce14caeaed7","fromNode":"41e0615c7f99db95","fromSide":"right","toNode":"374628eae111be73","toSide":"left"},
		{"id":"5a9525191f39e787","fromNode":"41e0615c7f99db95","fromSide":"right","toNode":"ea6c428f09925fce","toSide":"left"},
		{"id":"1b08cfadb23369fa","fromNode":"bc2ea0de7fa4ab05","fromSide":"bottom","toNode":"81396fbef5b6d2e4","toSide":"top"},
		{"id":"c8f4d515fdd84a50","fromNode":"c3b413fbc008f354","fromSide":"right","toNode":"c221ce6e83f88a43","toSide":"left"},
		{"id":"1828da9a3d8c8ef2","fromNode":"c221ce6e83f88a43","fromSide":"top","toNode":"be0ac761dd5178c6","toSide":"left"},
		{"id":"0b0f593a793e7eb0","fromNode":"c221ce6e83f88a43","fromSide":"right","toNode":"cb5fafbd55c722da","toSide":"left"},
		{"id":"6116be0e962d823e","fromNode":"cb5fafbd55c722da","fromSide":"right","toNode":"b4881964ab764cf4","toSide":"left"},
		{"id":"e434aad40057b485","fromNode":"cb5fafbd55c722da","fromSide":"right","toNode":"e325efe65cc6a0c5","toSide":"left"},
		{"id":"48f09b5b5a1a1fd3","fromNode":"36780efed2c38d2d","fromSide":"right","toNode":"9ecf6471d6862023","toSide":"left"},
		{"id":"09232497903596de","fromNode":"e325efe65cc6a0c5","fromSide":"right","toNode":"fe74750858ecfa37","toSide":"left"},
		{"id":"5df09d8fd420c8f0","fromNode":"cb5fafbd55c722da","fromSide":"right","toNode":"e49f538c54b0ffd9","toSide":"left"},
		{"id":"c8ae11bda8a8e2f3","fromNode":"e49f538c54b0ffd9","fromSide":"right","toNode":"69d4746c85e10248","toSide":"left"},
		{"id":"5b1601caae658dcb","fromNode":"e49f538c54b0ffd9","fromSide":"right","toNode":"36768f9e37437070","toSide":"left"},
		{"id":"76607bc44df5697e","fromNode":"e49f538c54b0ffd9","fromSide":"right","toNode":"0cc34c22e95d6033","toSide":"left"},
		{"id":"f5f965fc19a6e18c","fromNode":"c221ce6e83f88a43","fromSide":"right","toNode":"8b240aa476b1c1f9","toSide":"left"},
		{"id":"f9bf58b920b6e438","fromNode":"8b240aa476b1c1f9","fromSide":"right","toNode":"258baab9321bd97a","toSide":"left"},
		{"id":"42b4d933cbf28cf3","fromNode":"8b240aa476b1c1f9","fromSide":"right","toNode":"0e4a0621a6983cfb","toSide":"left"},
		{"id":"dc76ee4acb81e278","fromNode":"0e4a0621a6983cfb","fromSide":"right","toNode":"a4180979efaaac2d","toSide":"left"},
		{"id":"88740cd56b5a9f14","fromNode":"8b240aa476b1c1f9","fromSide":"right","toNode":"ef3350979e1ce992","toSide":"left"},
		{"id":"81b57c96e27a36b7","fromNode":"a4180979efaaac2d","fromSide":"right","toNode":"37e06b31a8831aa6","toSide":"left"},
		{"id":"895dfccd3024ce27","fromNode":"8b240aa476b1c1f9","fromSide":"bottom","toNode":"2d4f3132f0111986","toSide":"left"},
		{"id":"8920be44f9c3bb1c","fromNode":"2d4f3132f0111986","fromSide":"right","toNode":"871c17743541bc89","toSide":"left"},
		{"id":"b32400654bc79358","fromNode":"871c17743541bc89","fromSide":"right","toNode":"e56568966026ca84","toSide":"left","label":"ex.:"},
		{"id":"518898fb42daf776","fromNode":"2d4f3132f0111986","fromSide":"right","toNode":"db67b7757e2ed59b","toSide":"left"},
		{"id":"6b2afb21d3cd3d0f","fromNode":"871c17743541bc89","fromSide":"top","toNode":"e3959e78e56b4103","toSide":"left"},
		{"id":"0be1a09bf9d0487e","fromNode":"db67b7757e2ed59b","fromSide":"right","toNode":"ee3a7fe9dfc25c9b","toSide":"left"},
		{"id":"4e7c18f5a839289a","fromNode":"2d4f3132f0111986","fromSide":"bottom","toNode":"9f5df59f8deb1ccc","toSide":"left"},
		{"id":"ef3bc14da0cf2585","fromNode":"9f5df59f8deb1ccc","fromSide":"right","toNode":"3874d43e308ad199","toSide":"left"},
		{"id":"899c74736612274a","fromNode":"9f5df59f8deb1ccc","fromSide":"right","toNode":"fa472de1922e4f26","toSide":"left"},
		{"id":"7ab0445e025fa744","fromNode":"fa472de1922e4f26","fromSide":"right","toNode":"32733aa360e48970","toSide":"left"},
		{"id":"75a4205e4b6e5ec5","fromNode":"32733aa360e48970","fromSide":"right","toNode":"f8ce18f8df22c60a","toSide":"left","label":"ex.:"},
		{"id":"faac5ec81fe44f33","fromNode":"fa472de1922e4f26","fromSide":"right","toNode":"29d248a0c860e340","toSide":"left"},
		{"id":"b1cb7075527a9f96","fromNode":"29d248a0c860e340","fromSide":"right","toNode":"e7fc24f40f5b3fba","toSide":"left"},
		{"id":"3733a8796cdeef61","fromNode":"e7fc24f40f5b3fba","fromSide":"right","toNode":"383b1d4e133056b3","toSide":"left"},
		{"id":"65beb895266370e4","fromNode":"e7fc24f40f5b3fba","fromSide":"right","toNode":"4d0ef584a71d7db1","toSide":"left"},
		{"id":"36b601b991f1d937","fromNode":"f8ce18f8df22c60a","fromSide":"right","toNode":"e77d7f293b5eec5e","toSide":"left"},
		{"id":"cf3e4ef60000cc5f","fromNode":"3874d43e308ad199","fromSide":"right","toNode":"08fbca0f8b3a5cc1","toSide":"left"},
		{"id":"6f9be0285ee79956","fromNode":"3874d43e308ad199","fromSide":"right","toNode":"ced2b5c38ef92c04","toSide":"left"},
		{"id":"50c1c94c73bfd461","fromNode":"ced2b5c38ef92c04","fromSide":"right","toNode":"1004546655be250d","toSide":"left"},
		{"id":"a5924cba1e9bca08","fromNode":"08fbca0f8b3a5cc1","fromSide":"right","toNode":"767b64bfb5c23a20","toSide":"left","label":"ex.:"},
		{"id":"e6b49bb58631697a","fromNode":"767b64bfb5c23a20","fromSide":"right","toNode":"b153ab26a7ed737f","toSide":"left"},
		{"id":"32d36e9f57903b3a","fromNode":"2d4f3132f0111986","fromSide":"bottom","toNode":"1848302e29efcffe","toSide":"left"},
		{"id":"2eb168083715e9c8","fromNode":"1848302e29efcffe","fromSide":"top","toNode":"ec2a7824a99ae77f","toSide":"left"},
		{"id":"cc759402b2d927ef","fromNode":"1848302e29efcffe","fromSide":"right","toNode":"61a79ae1cf2e4c08","toSide":"left"},
		{"id":"18ca43ac9fcaca21","fromNode":"1848302e29efcffe","fromSide":"right","toNode":"7b622f64f0d9dae3","toSide":"left"},
		{"id":"5b7f4fc549f78e6e","fromNode":"1848302e29efcffe","fromSide":"right","toNode":"aaf8d853c9a80b4d","toSide":"left"},
		{"id":"5a25320cabab3b0f","fromNode":"1848302e29efcffe","fromSide":"bottom","toNode":"eec16f8adabcd2ef","toSide":"left","label":"ex.: "},
		{"id":"36f23363cf49c02e","fromNode":"eec16f8adabcd2ef","fromSide":"right","toNode":"80f1f24b115064bc","toSide":"left"},
		{"id":"65d7c369a3c5213b","fromNode":"eec16f8adabcd2ef","fromSide":"right","toNode":"acbe584dae339470","toSide":"left"},
		{"id":"673742c443c36d7f","fromNode":"eec16f8adabcd2ef","fromSide":"right","toNode":"c8b6f4b70777c484","toSide":"left"},
		{"id":"3e8f8a78b3a8d9ea","fromNode":"eec16f8adabcd2ef","fromSide":"right","toNode":"4063e89cbf7fa2c2","toSide":"left"},
		{"id":"4f2d28c5002f0315","fromNode":"61a79ae1cf2e4c08","fromSide":"right","toNode":"655b2e3dd913072e","toSide":"left"},
		{"id":"d0ffdea5ba35eb64","fromNode":"eec16f8adabcd2ef","fromSide":"bottom","toNode":"c02c2051555cfdc6","toSide":"left","label":"probabilidades"},
		{"id":"3f2586b65811b8c3","fromNode":"3d3621cd0785f49f","fromSide":"right","toNode":"f40b31d2df6c2571","toSide":"left"},
		{"id":"4c4b6fc84f26f6d7","fromNode":"ec2a7824a99ae77f","fromSide":"right","toNode":"1187b72eed9b3ca9","toSide":"left"},
		{"id":"42b3431d16468de9","fromNode":"7b622f64f0d9dae3","fromSide":"right","toNode":"43688a7c471368bc","toSide":"left"},
		{"id":"199f2b358b4db1ad","fromNode":"aaf8d853c9a80b4d","fromSide":"right","toNode":"ba98656cc5d40925","toSide":"left"},
		{"id":"e99c2e3afcaabc1b","fromNode":"f40b31d2df6c2571","fromSide":"right","toNode":"fbbdaa12685b0629","toSide":"left"},
		{"id":"0e2141b1f2891075","fromNode":"f40b31d2df6c2571","fromSide":"right","toNode":"e87bfbb4587a0a59","toSide":"left"},
		{"id":"3e61575da2192850","fromNode":"f40b31d2df6c2571","fromSide":"right","toNode":"4fe51916843807ed","toSide":"left"},
		{"id":"6db30c91b34d8a32","fromNode":"f40b31d2df6c2571","fromSide":"bottom","toNode":"5ec1e06ce37e90b1","toSide":"left","label":"Treinamento"},
		{"id":"0d70d2fea40fd223","fromNode":"5ec1e06ce37e90b1","fromSide":"right","toNode":"3ad942bbd8e16774","toSide":"left"},
		{"id":"171ce6e8cc0e4a12","fromNode":"5ec1e06ce37e90b1","fromSide":"right","toNode":"e509286763d3e3b3","toSide":"left"},
		{"id":"e679b90c51f06297","fromNode":"5ec1e06ce37e90b1","fromSide":"right","toNode":"b7e06b5861e4920a","toSide":"left"},
		{"id":"0b71ec29cce26dcb","fromNode":"5ec1e06ce37e90b1","fromSide":"right","toNode":"6220dafe72fcac03","toSide":"left"},
		{"id":"097f32e9e67ab205","fromNode":"3ad942bbd8e16774","fromSide":"right","toNode":"5afe45003c964137","toSide":"left"},
		{"id":"961b63e1a1d38b22","fromNode":"f40b31d2df6c2571","fromSide":"bottom","toNode":"87372b99c8a1fc40","toSide":"left"},
		{"id":"e710c15248a6b011","fromNode":"87372b99c8a1fc40","fromSide":"right","toNode":"0511fed77ad19207","toSide":"left"},
		{"id":"c0bfe072f8b7edd4","fromNode":"87372b99c8a1fc40","fromSide":"right","toNode":"1045031e4b8420ca","toSide":"left"},
		{"id":"7ca7332bd0f3682f","fromNode":"87372b99c8a1fc40","fromSide":"right","toNode":"78ecd9316886a10a","toSide":"left"},
		{"id":"8b731fa1a877ef99","fromNode":"f40b31d2df6c2571","fromSide":"bottom","toNode":"b26d734b4e922f53","toSide":"left"},
		{"id":"03dd13c2b5cfb02c","fromNode":"b26d734b4e922f53","fromSide":"right","toNode":"5d9137eb9ad70526","toSide":"left"},
		{"id":"39629a10752bff58","fromNode":"b26d734b4e922f53","fromSide":"right","toNode":"8592b1c8548ea48d","toSide":"left"},
		{"id":"1b0fcb4051b4616a","fromNode":"b26d734b4e922f53","fromSide":"right","toNode":"29dfe07860b0e1e8","toSide":"left"},
		{"id":"a00909e0688cbbb5","fromNode":"3d3621cd0785f49f","fromSide":"right","toNode":"96093af4e6f8c788","toSide":"left"},
		{"id":"8bc959f035899231","fromNode":"96093af4e6f8c788","fromSide":"right","toNode":"9145f41d657a8e68","toSide":"left"},
		{"id":"3bb457e7e1048cc6","fromNode":"96093af4e6f8c788","fromSide":"right","toNode":"5410bc67e4c8cc6f","toSide":"left"},
		{"id":"c5396d74ffa7828e","fromNode":"96093af4e6f8c788","fromSide":"right","toNode":"647e132fec00ca08","toSide":"left"},
		{"id":"2deed63de5137829","fromNode":"96093af4e6f8c788","fromSide":"right","toNode":"9f77d67c4ab4da80","toSide":"left"}
	]
}