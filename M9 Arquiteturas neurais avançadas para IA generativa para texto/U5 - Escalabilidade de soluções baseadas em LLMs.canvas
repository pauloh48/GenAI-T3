{
	"nodes":[
		{"id":"83d2eaa859718372","type":"text","text":"# U5. Escalabilidade de soluções baseadas em LLMs","x":-420,"y":-27,"width":420,"height":87},
		{"id":"e3d89b7d3a45ea0e","type":"text","text":"\n\n- **Definição:** capacidade do sistema de **crescer e processar maior número de requisições** sem comprometer o desempenho, a estabilidade ou a qualidade das respostas.\n- **Meta:** garantir que as soluções **atendam mais usuários simultaneamente**, mantendo **baixa latência** e **custos controlados**, mesmo em situações de alta demanda.\n","x":160,"y":-480,"width":740,"height":117},
		{"id":"a8aac171cbde0537","type":"text","text":"- **Desafios comuns:**\n    - **Gerenciar custos computacionais:** controlar o uso de GPU, tokens e requisições para evitar desperdício.\n    - **Manter eficiência em larga escala:** aplicar cache, paralelização e balanceamento de carga para otimizar o desempenho.\n    - **Adotar infraestrutura elástica:** integrar com plataformas de nuvem e containers (como Kubernetes ou Docker) para ajustar recursos conforme a demanda.\n    - **Garantir estabilidade e segurança:** prevenir gargalos, falhas e degradação do serviço durante picos de uso.","x":160,"y":-321,"width":720,"height":238},
		{"id":"1ef55b01d005c4d4","type":"text","text":" **Demandas Computacionais dos LLMs**\n- LLMs são modelos **de grande porte** que exigem muito poder de processamento.\n- O uso de **GPUs** (Graphics Processing Units) ou **TPUs** (Tensor Processing Units) é fundamental para lidar com cálculos complexos.","x":980,"y":-483,"width":711,"height":123},
		{"id":"cf6f73684caad709","type":"text","text":" **Demandas Computacionais dos LLMs**\n- LLMs são modelos **de grande porte** que exigem muito poder de processamento.\n- O uso de **GPUs** (Graphics Processing Units) ou **TPUs** (Tensor Processing Units) é fundamental para lidar com cálculos complexos.\n    \n","x":1920,"y":-840,"width":725,"height":144},
		{"id":"86f3a589bf44f1a5","type":"text","text":" **Aspectos Técnicos da Escalabilidade**\n- **Planejamento inicial:** pensar na escalabilidade desde o design da solução.\n- **Balanceamento de carga:** distribui as requisições entre servidores, evitando sobrecarga.\n- **Latência:** deve ser minimizada para garantir respostas rápidas ao usuário.\n- **Infraestrutura elástica:** permite aumentar ou reduzir recursos conforme a demanda.\n","x":1920,"y":-680,"width":725,"height":163},
		{"id":"8359483d469ae948","type":"text","text":" **Desafios Operacionais**\n- Gerenciar **recursos limitados** de hardware e energia.\n- **Monitorar o desempenho** contínuo para evitar gargalos.\n- Lidar com **custos crescentes** conforme o sistema é ampliado.","x":1920,"y":-497,"width":725,"height":137},
		{"id":"c74bac8ca72e9f1f","type":"text","text":" **Custos e Sustentabilidade**\n- Escalar um sistema LLM pode gerar **altos custos** com:\n    - Hardware (GPUs, TPUs);\n    - Energia elétrica;\n    - Manutenção e atualização da infraestrutura.\n- É importante buscar **eficiência** para equilibrar desempenho e custo.","x":1920,"y":-340,"width":725,"height":200},
		{"id":"818ce7383839174c","type":"text","text":"## Capacidade Computacional","x":220,"y":141,"width":260,"height":79},
		{"id":"d6a35d684eeea532","type":"text","text":" **Planejamento da Infraestrutura**\n\n- Adotar LLMs exige **planejamento cuidadoso** de hardware e recursos.\n- Se a empresa **superdimensionar**, gasta mais do que o necessário.\n- Se **subdimensionar**, o modelo ficará lento ou instável.\n- O objetivo é encontrar o **equilíbrio entre custo e desempenho**, garantindo eficiência e retorno do investimento.","x":612,"y":23,"width":928,"height":158},
		{"id":"c027f8b40b121cca","type":"text","text":" **O Que Deve Ser Avaliado**\n- **Capacidade computacional disponível** (GPUs, memória, rede).\n- **Desempenho** em diferentes cargas de trabalho.\n- **Custos operacionais** (energia, manutenção e upgrades).\n- **Estratégias de otimização**, que variam conforme o porte da empresa e o tipo de aplicação.","x":612,"y":200,"width":928,"height":165},
		{"id":"7cc9f256c3e2591c","type":"text","text":" **Consumo de Memória da GPU**\n- Cada GPU tem uma quantidade de **VRAM (memória de vídeo)**.\n- Essa memória é usada para armazenar:\n    - Os **parâmetros do modelo** (os “pesos” aprendidos);\n    - Os **dados temporários** necessários durante a execução.\n- Modelos grandes, como o **GPT-3**, podem ocupar **dezenas de gigabytes de VRAM**.\n- Se a memória for insuficiente, o sistema pode falhar ou interromper o processamento.\n- É possível dividir o modelo em várias GPUs, mas isso exige **configuração precisa** para evitar gargalos.","x":1210,"y":400,"width":793,"height":258},
		{"id":"a360518c7bc1ab38","type":"text","text":" **Latência de Inferência**\n- **Latência** é o tempo entre a pergunta do usuário e a resposta do modelo.\n- Em aplicações como **chatbots** ou **sistemas de e-commerce**, o tempo de resposta é essencial.\n- Estudos mostram que modelos maiores têm **latência maior** e exigem **mais poder computacional** (Brown et al., 2020).\n- Portanto, empresas precisam equilibrar **acurácia da resposta** e **velocidade de execução**.","x":1210,"y":678,"width":797,"height":193},
		{"id":"7501f1a6d792d0db","type":"text","text":" **Tamanho do Modelo x Desempenho**\n- **Modelos grandes:** respostas mais precisas, porém com maior custo e lentidão.\n- **Modelos menores:** respostas mais rápidas e baratas, mas com menor precisão.\n- A escolha depende do **cenário de uso**:\n    - Para sistemas internos, um modelo menor pode ser suficiente.\n    - Para aplicações críticas (como atendimento ao público), pode valer a pena usar um modelo maior.","x":2074,"y":538,"width":816,"height":199},
		{"id":"125600e2fe1751ac","type":"text","text":"### Métricas de Capacidade Computacional para LLMs","x":750,"y":619,"width":326,"height":79},
		{"id":"4eb1585d850a22be","type":"text","text":"### Implantação do _LLM_","x":468,"y":1160,"width":282,"height":60},
		{"id":"cf8c0089d881a848","type":"text","text":"* Implantar um LLM em uma empresa exige avaliar **desempenho, custo e viabilidade técnica**.  \n","x":609,"y":920,"width":727,"height":64},
		{"id":"b41fddde9b33ea80","type":"text","text":"* Há **quatro principais abordagens** para colocar soluções com LLMs em produção, que variam em **complexidade, custo e controle**:","x":740,"y":1025,"width":596,"height":55},
		{"id":"a0fd24b7ebc64c7f","type":"text","text":"* Quatro abordagens para colocar _Large Language Models_ em produção\n![[abordagens.png]]","x":1160,"y":1100,"width":643,"height":411},
		{"id":"252baeea1df5f0eb","type":"text","text":" **FTaaS (Fine-Tuning-as-a-Service)**\n- Permite realizar o ajuste fino em **infraestrutura de terceiros**.\n- Reduz a necessidade de manutenção e hardware interno.\n- O custo é proporcional ao **uso e volume de tokens processados**.","x":3100,"y":1323,"width":540,"height":140},
		{"id":"d37312ff6d7b9da5","type":"text","text":" **Engenharia de Prompt com Contexto**\n- **Mais simples e acessível.**\n- Consiste em criar **prompts bem estruturados** e enviá-los a APIs de LLMs (ex.: GPT-4®, Gemini®).\n- Não requer infraestrutura própria nem equipe técnica avançada.\n- **Vantagens:** baixo custo, alta escalabilidade e rápida implementação.\n- **Limitações:** pouca personalização e custo variável conforme o uso da API.\n- **Ideal para:** empresas iniciando na área ou que precisam de resultados rápidos.","x":2120,"y":920,"width":860,"height":202},
		{"id":"5ef5fddaa059d4bd","type":"text","text":" **RAG (Retrieval-Augmented Generation)**\n- Adiciona uma camada de **recuperação de informações externas** (documentos, bases internas, sites).\n- O modelo recebe o **contexto relevante** antes de gerar a resposta.\n- **Melhora a precisão** e atualiza o conhecimento do modelo sem necessidade de retreinamento.\n- **Requisitos:** integração com bancos de dados, mecanismos de busca e infraestrutura de indexação.\n- **Vantagens:** respostas mais precisas e atualizadas.\n- **Limitações:** custo e complexidade maiores que a engenharia de prompt.\n- **Ideal para:** aplicações que precisam de informações específicas e dinâmicas (ex.: atendimento corporativo).","x":2120,"y":1136,"width":860,"height":240},
		{"id":"f1665097fd092466","type":"text","text":" **Ajuste Fino (Fine-tuning)**\n- Envolve **re-treinar o modelo** com dados específicos da empresa.\n- Pode ser feito:\n    - Com **GPU própria**;\n    - Em **serviços terceirizados** (ex.: Together.ai®);\n    - Por **plataformas de Fine-Tuning-as-a-Service (FTaaS)** (ex.: OpenAI®).\n- **Vantagens:** respostas mais adaptadas ao domínio da empresa.\n- **Limitações:** custo alto, necessidade de expertise técnica e tempo de processamento.\n- **Ideal para:** empresas que precisam de respostas altamente especializadas e consistentes com o seu contexto.","x":2120,"y":1393,"width":860,"height":283},
		{"id":"bb3d27a1b44abe3e","type":"text","text":"**Modelos de Código Aberto e Treinamento Próprio**","x":2270,"y":1816,"width":250,"height":59},
		{"id":"dec2d5aa764f9f4e","type":"text","text":"1. **Modelo local (on-premise):**\n    - O LLM roda na infraestrutura da empresa.\n    - Garante **total controle e segurança dos dados**, mas exige GPUs caras e equipe técnica.\n    - Comum em **órgãos públicos e empresas que tratam dados sensíveis**.","x":2720,"y":1717,"width":714,"height":129},
		{"id":"b100417db82ef6f8","type":"text","text":"2. **Modelo em plataformas (ex.: Together.ai®):**\n    - O modelo é acessado via API, sem necessidade de hardware próprio.\n    - **Mais simples de escalar**, porém com **menor controle sobre os dados**.","x":2720,"y":1865,"width":714,"height":98},
		{"id":"1866a0a1bc76df60","type":"text","text":"3. **Modelo com ajuste fino local:**\n    - A empresa realiza o fine-tuning e a inferência internamente.\n    - Oferece **melhor personalização**, mas requer **alto poder computacional** e manutenção contínua.\n","x":2720,"y":1983,"width":714,"height":137},
		{"id":"7f3ad6229febba28","type":"text","text":"4. **Treinamento do próprio modelo:**\n    - O modelo é desenvolvido **do zero**, com grandes volumes de dados e clusters de GPUs/TPUs.\n    - É a opção **mais cara e complexa**, viável apenas para grandes corporações.\n    - Exemplo: o **GPT-4®** da OpenAI® custou mais de **US$ 100 milhões** em computação (Knight, 2023).","x":2720,"y":2140,"width":714,"height":177},
		{"id":"559419735ce495d5","type":"text","text":" **Tamanho do Modelo e Memória Necessária**\n\n- O **tamanho do modelo** (número de parâmetros) é o principal fator que determina a **quantidade de memória (VRAM)** exigida.\n- **Modelos grandes**, como o _Llama-3-70B®_ (com 70 bilhões de parâmetros), exigem **cerca de 150 GB de memória** para funcionar.\n    \n- **Regra geral:**\n    - Modelos **acima de 10 bilhões de parâmetros** exigem **GPUs com pelo menos 24 GB de VRAM**.\n    - Exemplos adequados: **NVIDIA A100®** ou **RTX 4090®**.\n    - GPUs menores (8–16 GB) não suportam esses modelos.","x":1316,"y":1814,"width":804,"height":266},
		{"id":"29a49342b64f7abf","type":"text","text":"#### Requisitos de GPUs para Implantação de LLMs em Infraestrutura Local","x":771,"y":1926,"width":404,"height":76},
		{"id":"ed7b2886ea70b9b3","type":"text","text":"**Como Calcular a Memória Necessária**\nA **memória total** usada pelo modelo depende de quatro elementos:\n1. **Parâmetros (P):** número total de pesos do modelo.\n2. **Bytes por parâmetro (B):** cada parâmetro ocupa 4 bytes (32 bits).\n3. **Precisão de carregamento (Q):** geralmente 16 bits, mas pode ser menor (com quantização).\n4. **Fator de sobrecarga (1.2):** acréscimo de 20% para ativações, cache e operações temporárias.","x":1316,"y":2200,"width":804,"height":200},
		{"id":"ff097ac1d0ee82c9","type":"text","text":"![[eq_mem.png]]","x":2177,"y":2400,"width":686,"height":121},
		{"id":"3fd748286e3275d3","type":"text","text":"**Exemplo:**  \nO _Llama-3-70B®_ em 16 bits precisa de **aproximadamente 168 GB de VRAM** — ou seja, **mais de duas GPUs A100 de 80 GB**.","x":3020,"y":2346,"width":920,"height":74},
		{"id":"915728dd6e1a6a29","type":"text","text":" **Quantização: Redução da Memória**\n- **Quantização** é o processo de reduzir a **precisão dos parâmetros** do modelo (por exemplo, de 16 bits para 8 ou 4 bits).\n- **Objetivo:** diminuir o uso de memória, permitindo que modelos grandes rodem em máquinas mais modestas.\n- **Impacto:** pequena perda de precisão e desempenho, dependendo do nível de redução.\n\n","x":3020,"y":2440,"width":920,"height":140},
		{"id":"0b246d9439543369","type":"text","text":"**Exemplos:**\n\n- **16 bits:** cerca de **168 GB** de VRAM (alto desempenho).\n    \n- **8 bits:** mantém desempenho similar com **metade da memória**.\n    \n- **4 bits:** reduz a necessidade para cerca de **42 GB**, possibilitando rodar o modelo em **duas GPUs de 24 GB (ex.: L4)**.","x":4040,"y":2444,"width":899,"height":132},
		{"id":"5b0cb527e418151d","type":"text","text":"* **Estimativa de Memória de GPU para Diferentes LLMs**\n\n| **Modelo**               | **Parâmetros (bilhões)** | **Precisão (bits)** | **Memória Estimada (GB)** | **Exemplo de Configuração Viável** | **Observações**                 |\n| ------------------------ | ------------------------ | ------------------- | ------------------------- | ---------------------------------- | ------------------------------- |\n| **GPT-2 (1.5B)**         | 1,5                      | 16                  | ~3,6 GB                   | 1× RTX 3060 (12 GB)                | Roda facilmente em GPU comum    |\n| **Llama-2-7B**           | 7                        | 16                  | ~17 GB                    | 1× RTX 3090 / RTX 4090 (24 GB)     | Adequado para uso local simples |\n| **Llama-2-13B**          | 13                       | 16                  | ~31 GB                    | 2× RTX 4090 (24 GB cada)           | Requer divisão entre GPUs       |\n| **Llama-3-70B**          | 70                       | 16                  | ~168 GB                   | 3× A100 (80 GB cada)               | Exige cluster de GPUs           |\n| **GPT-3 (175B)**         | 175                      | 16                  | ~420 GB                   | 6× A100 (80 GB cada)               | Usado apenas em data centers    |\n| **Llama-3-70B (8 bits)** | 70                       | 8                   | ~84 GB                    | 2× A100 (80 GB) ou 4× RTX 4090     | Reduz memória pela metade       |\n| **Llama-3-70B (4 bits)** | 70                       | 4                   | ~42 GB                    | 2× L4 (24 GB)                      | Perde um pouco de precisão      |\n| **Mistral-7B (4 bits)**  | 7                        | 4                   | ~4,2 GB                   | 1× RTX 3060 / T4                   | Ideal para testes locais leves  |\n","x":3020,"y":2620,"width":1200,"height":360},
		{"id":"eaec01659b019293","type":"text","text":"## Otimização de Latência e Acurácia","x":7,"y":2477,"width":293,"height":83},
		{"id":"c9779b076a2f8fbc","type":"text","text":"* A otimização busca equilibrar **velocidade de resposta (latência)** e **qualidade das respostas (acurácia)**.  \n* Ambos os fatores influenciam diretamente a **experiência do usuário** e a **eficiência operacional** em aplicações reais.","x":203,"y":2336,"width":797,"height":84},
		{"id":"e3069e697d029cf5","type":"text","text":"#### Otimização de Latência","x":399,"y":2533,"width":301,"height":55},
		{"id":"17e946bab0e6edb0","type":"text","text":" **Objetivo**\n* Reduzir o tempo de resposta do modelo sem comprometer a qualidade das respostas.","x":799,"y":2483,"width":721,"height":78},
		{"id":"d2587f6cf143946b","type":"text","text":"|**Estratégia**|**Descrição**|**Benefício**|\n|---|---|---|\n|**Uso de modelos menores**|Modelos com menos parâmetros consomem menos GPU e respondem mais rápido.|Reduz o tempo de inferência.|\n|**Cache de respostas**|Armazenar resultados de consultas repetidas (por exemplo, perguntas frequentes).|Evita recomputar respostas e diminui a latência.|\n|**Redução de tokens de entrada e saída**|Enviar apenas o essencial no prompt e limitar o tamanho da resposta.|Menos tokens = respostas mais rápidas e baratas.|\n|**Agrupamento de solicitações**|Unir várias tarefas em uma única chamada à API.|Reduz latência acumulada de múltiplas requisições.|\n|**Execução paralela**|Rodar chamadas simultaneamente quando as tarefas são independentes.|Melhora o tempo total de processamento.|\n|**Streaming de resposta**|Exibir o texto conforme o modelo o gera.|Melhora a percepção de velocidade pelo usuário.|\n|**Interface responsiva**|Usar indicadores visuais (barras de progresso, animações).|Melhora a experiência de espera.|","x":818,"y":2612,"width":1359,"height":268},
		{"id":"3ca9d877c6c77995","type":"text","text":"#### Otimização de Acurácia","x":451,"y":3040,"width":301,"height":55},
		{"id":"08c96e39eec8b680","type":"text","text":" **Dados Importantes (segundo OpenAI®, 2024)**\n- Reduzir **50% dos tokens de saída** → reduz a latência em até **50%**.\n- Reduzir tokens de entrada → impacto menor (1% a 5%).\n- Configurar prompts para respostas curtas (“em até 20 palavras”) ajuda a economizar tempo e custo.","x":2250,"y":2679,"width":541,"height":135},
		{"id":"d069cb94660f9842","type":"text","text":" **Objetivo**\n- Melhorar a **relevância, consistência e confiabilidade** das respostas geradas.","x":837,"y":2982,"width":603,"height":86},
		{"id":"eecdbf72256fbe96","type":"text","text":"* Matriz de otimização da acurácia de _Large Language Models_*\n![[mat_otimizacao.png]]","x":960,"y":3095,"width":659,"height":331},
		{"id":"43e0f6b62ede4a12","x":1693,"y":3068,"width":968,"height":174,"type":"text","text":"* Dimensões de Otimização\n\n| **Dimensão**               | **Foco**                     | **Exemplo de Técnica**                                        |\n| -------------------------- | ---------------------------- | ------------------------------------------------------------- |\n| **Otimização de contexto** | O que o modelo precisa saber | Recuperação de documentos (RAG), adição de contexto ao prompt |\n| **Otimização do modelo**   | Como o modelo deve agir      | Ajuste fino, exemplos (few-shot), verificação de saída        |"},
		{"id":"b761a8a11b8d7b58","x":1693,"y":3261,"width":968,"height":197,"type":"text","text":"**Etapas Recomendadas (Fluxo Didático)**\n\n1. **Comece com um prompt simples** e observe o resultado.\n    \n2. **Adicione exemplos estáticos** (few-shot learning) para melhorar a consistência.\n    \n3. **Implemente RAG** para trazer informações externas e aumentar a precisão.\n    \n4. **Realize ajuste fino (fine-tuning)** com dados específicos da empresa (≥50 exemplos).\n    \n5. **Adicione verificações automáticas de saída**, evitando respostas incorretas ou alucinações.\n    \n6. **Re-treine periodicamente** com novos dados e casos de erro para evoluir o desempenho."}
	],
	"edges":[
		{"id":"44650579e09b1a9a","fromNode":"83d2eaa859718372","fromSide":"top","toNode":"e3d89b7d3a45ea0e","toSide":"left"},
		{"id":"351bcc55369c4101","fromNode":"83d2eaa859718372","fromSide":"top","toNode":"a8aac171cbde0537","toSide":"left"},
		{"id":"d0f0ece68951a2fd","fromNode":"e3d89b7d3a45ea0e","fromSide":"right","toNode":"1ef55b01d005c4d4","toSide":"left"},
		{"id":"ac76ac48b4c57de4","fromNode":"1ef55b01d005c4d4","fromSide":"right","toNode":"cf6f73684caad709","toSide":"left"},
		{"id":"d81810ef35eb527e","fromNode":"1ef55b01d005c4d4","fromSide":"right","toNode":"86f3a589bf44f1a5","toSide":"left"},
		{"id":"f8d8e262de5371fc","fromNode":"1ef55b01d005c4d4","fromSide":"right","toNode":"8359483d469ae948","toSide":"left"},
		{"id":"54be72c1b1116348","fromNode":"1ef55b01d005c4d4","fromSide":"right","toNode":"c74bac8ca72e9f1f","toSide":"left"},
		{"id":"ab4d53eb36437bcf","fromNode":"83d2eaa859718372","fromSide":"right","toNode":"818ce7383839174c","toSide":"left"},
		{"id":"6f3b0d13a8fd0390","fromNode":"818ce7383839174c","fromSide":"right","toNode":"d6a35d684eeea532","toSide":"left"},
		{"id":"700581cfd768300d","fromNode":"818ce7383839174c","fromSide":"right","toNode":"c027f8b40b121cca","toSide":"left"},
		{"id":"05754d6866d38046","fromNode":"818ce7383839174c","fromSide":"bottom","toNode":"125600e2fe1751ac","toSide":"left"},
		{"id":"578beda64f837bb3","fromNode":"125600e2fe1751ac","fromSide":"right","toNode":"7cc9f256c3e2591c","toSide":"left"},
		{"id":"97ade7d5bc01a5f1","fromNode":"125600e2fe1751ac","fromSide":"right","toNode":"a360518c7bc1ab38","toSide":"left"},
		{"id":"5b277a6d7bc6055a","fromNode":"7cc9f256c3e2591c","fromSide":"right","toNode":"7501f1a6d792d0db","toSide":"left"},
		{"id":"4647acb87a1e7f4e","fromNode":"a360518c7bc1ab38","fromSide":"right","toNode":"7501f1a6d792d0db","toSide":"left"},
		{"id":"3b0e4607576d975e","fromNode":"818ce7383839174c","fromSide":"bottom","toNode":"4eb1585d850a22be","toSide":"left"},
		{"id":"ceac87488eb53d28","fromNode":"4eb1585d850a22be","fromSide":"right","toNode":"b41fddde9b33ea80","toSide":"left"},
		{"id":"5d94ef151fd02aa7","fromNode":"4eb1585d850a22be","fromSide":"top","toNode":"cf8c0089d881a848","toSide":"left"},
		{"id":"f7b43abaa2950241","fromNode":"b41fddde9b33ea80","fromSide":"bottom","toNode":"a0fd24b7ebc64c7f","toSide":"left"},
		{"id":"e75876b6e598e589","fromNode":"a0fd24b7ebc64c7f","fromSide":"right","toNode":"d37312ff6d7b9da5","toSide":"left"},
		{"id":"6d1ed93d74d3dac8","fromNode":"a0fd24b7ebc64c7f","fromSide":"right","toNode":"5ef5fddaa059d4bd","toSide":"left"},
		{"id":"05f8d1dff03aef15","fromNode":"a0fd24b7ebc64c7f","fromSide":"right","toNode":"f1665097fd092466","toSide":"left"},
		{"id":"c7cf56e5a584abfb","fromNode":"f1665097fd092466","fromSide":"right","toNode":"252baeea1df5f0eb","toSide":"left"},
		{"id":"c3eb74e0823bc995","fromNode":"a0fd24b7ebc64c7f","fromSide":"right","toNode":"bb3d27a1b44abe3e","toSide":"left"},
		{"id":"fb84e12da08b4316","fromNode":"bb3d27a1b44abe3e","fromSide":"right","toNode":"dec2d5aa764f9f4e","toSide":"left"},
		{"id":"9680bb0191a8ef20","fromNode":"bb3d27a1b44abe3e","fromSide":"right","toNode":"b100417db82ef6f8","toSide":"left"},
		{"id":"df792b1c331dc9c4","fromNode":"bb3d27a1b44abe3e","fromSide":"right","toNode":"1866a0a1bc76df60","toSide":"left"},
		{"id":"e9277a94033a0878","fromNode":"bb3d27a1b44abe3e","fromSide":"right","toNode":"7f3ad6229febba28","toSide":"left"},
		{"id":"b2c9c3036dc67e81","fromNode":"4eb1585d850a22be","fromSide":"right","toNode":"29a49342b64f7abf","toSide":"left"},
		{"id":"f8e0ac980d6a582b","fromNode":"29a49342b64f7abf","fromSide":"right","toNode":"559419735ce495d5","toSide":"left"},
		{"id":"e92a5809ac0204b5","fromNode":"29a49342b64f7abf","fromSide":"right","toNode":"ed7b2886ea70b9b3","toSide":"left"},
		{"id":"414bd9b902e958db","fromNode":"ed7b2886ea70b9b3","fromSide":"right","toNode":"ff097ac1d0ee82c9","toSide":"left"},
		{"id":"fcafb53b3af1b9c5","fromNode":"ff097ac1d0ee82c9","fromSide":"right","toNode":"3fd748286e3275d3","toSide":"left"},
		{"id":"4c55490c76616347","fromNode":"ff097ac1d0ee82c9","fromSide":"right","toNode":"915728dd6e1a6a29","toSide":"left"},
		{"id":"a28133a0dc0b1b0c","fromNode":"915728dd6e1a6a29","fromSide":"right","toNode":"0b246d9439543369","toSide":"left"},
		{"id":"66d5e57100fc8cc7","fromNode":"ff097ac1d0ee82c9","fromSide":"right","toNode":"5b0cb527e418151d","toSide":"left"},
		{"id":"cf2beaee904b73b6","fromNode":"83d2eaa859718372","fromSide":"bottom","toNode":"eaec01659b019293","toSide":"left"},
		{"id":"a6ae1bf235eea1b5","fromNode":"eaec01659b019293","fromSide":"top","toNode":"c9779b076a2f8fbc","toSide":"left"},
		{"id":"c7954a39c2af59e6","fromNode":"eaec01659b019293","fromSide":"right","toNode":"e3069e697d029cf5","toSide":"left"},
		{"id":"b37a3042093c48af","fromNode":"e3069e697d029cf5","fromSide":"right","toNode":"17e946bab0e6edb0","toSide":"left"},
		{"id":"f60500e885561e3a","fromNode":"e3069e697d029cf5","fromSide":"right","toNode":"d2587f6cf143946b","toSide":"left"},
		{"id":"d6142ed6bb19251a","fromNode":"eaec01659b019293","fromSide":"right","toNode":"3ca9d877c6c77995","toSide":"left"},
		{"id":"f18cbc0910215726","fromNode":"d2587f6cf143946b","fromSide":"right","toNode":"08c96e39eec8b680","toSide":"left"},
		{"id":"6956f2eb6debc1fa","fromNode":"3ca9d877c6c77995","fromSide":"right","toNode":"d069cb94660f9842","toSide":"left"},
		{"id":"1c053a7f2861206e","fromNode":"3ca9d877c6c77995","fromSide":"right","toNode":"eecdbf72256fbe96","toSide":"left"},
		{"id":"4395fc15ae4c0cb5","fromNode":"eecdbf72256fbe96","fromSide":"right","toNode":"43e0f6b62ede4a12","toSide":"left"},
		{"id":"906359b636684b6a","fromNode":"eecdbf72256fbe96","fromSide":"right","toNode":"b761a8a11b8d7b58","toSide":"left"}
	]
}