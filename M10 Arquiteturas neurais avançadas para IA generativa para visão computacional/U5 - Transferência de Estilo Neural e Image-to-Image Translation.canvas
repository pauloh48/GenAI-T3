{
	"nodes":[
		{"id":"9c97701ce2294a94","type":"text","text":"**Objetivo da Unidade**\n- Entender como **separar e recombinar conteúdo e estilo** em imagens.\n- Aprender a **traduzir imagens entre domínios diferentes**, mantendo a estrutura principal e alterando apenas o estilo visual.\n- Saber **quando e como aplicar** diferentes abordagens (otimização, feedforward, GANs e difusão).","x":-140,"y":-380,"width":940,"height":140},
		{"id":"6f4f949fe721bfc7","type":"text","text":"## Método de Gatys para Transferência de Estilo Neural","x":-20,"y":-20,"width":389,"height":91},
		{"id":"cfa9625eec49d937","type":"text","text":"**Ideia Central**\n\n- Proposto por **Leon Gatys et al. (2016)**, o método mostrou que redes neurais convolucionais (CNNs) podem **separar conteúdo e estilo** de imagens.\n    \n- Camadas **profundas da CNN** capturam **conteúdo e estrutura** (formas e objetos).\n    \n- Camadas **superficiais** capturam **estilo** (cores, texturas e traços artísticos).","x":540,"y":-158,"width":1103,"height":138},
		{"id":"efba9e77dafedac4","type":"text","text":"**Como Funciona**\n1. Usa-se uma **CNN pré-treinada**, geralmente a **VGG-19®**, apenas como **extratora de características** (sem ajuste de pesos).\n2. Define-se:\n    - **Imagem de conteúdo** → fornece a estrutura (ex.: uma fotografia).\n    - **Imagem de estilo** → fornece o padrão visual (ex.: pintura de Van Gogh).\n3. Calcula-se:\n    - **Representação de conteúdo** → ativações nas camadas profundas da VGG.\n    - **Representação de estilo** → **matrizes de Gram**, que registram as correlações entre os mapas de ativação (as “assinaturas” de textura e cor).\n4. Inicia-se uma **imagem de saída** (ruído ou a própria foto) e, por meio de **otimização iterativa**, ajustam-se seus pixels para:\n    - Preservar o conteúdo da foto.\n    - Reproduzir o estilo da pintura.","x":540,"y":6,"width":1103,"height":334},
		{"id":"066be457e5668626","type":"text","text":"**Limitações e Evoluções**\n\n- O método original é **computacionalmente caro**, pois cada nova combinação (conteúdo + estilo) requer **otimização lenta**.\n    \n- Isso levou ao desenvolvimento de **métodos acelerados**:\n    \n    - **Redes Feedforward (Johnson et al., 2016):** aplicam o estilo em tempo real.\n        \n    - **AdaIN (Ulyanov et al., 2017):** ajusta estatísticas de forma dinâmica para múltiplos estilos.\n        \n- Hoje, essas técnicas são amplamente usadas em **filtros artísticos, editores de imagem e ferramentas criativas de IA**.","x":540,"y":360,"width":1103,"height":200},
		{"id":"729d3196fde61a0e","type":"text","text":"**Conceito de Matriz de Gram**\n\n- Representa **como os padrões da rede se relacionam entre si**, sem considerar a posição exata dos objetos.\n    \n- Captura a **“essência estética”** da imagem — suas cores, texturas e pinceladas.\n    \n- É o que permite que uma **foto urbana** se transforme em uma **obra no estilo Van Gogh**, mantendo a estrutura da cidade, mas com o estilo de pintura artística.","x":1760,"y":-7,"width":827,"height":156},
		{"id":"9387e10b6f899443","type":"text","text":"![[gatys.png]]","x":1800,"y":160,"width":600,"height":400},
		{"id":"9fcbe3b2272e3279","type":"text","text":"# U5. Transferência de Estilo Neural e Image-to-Image Translation","x":-680,"y":173,"width":495,"height":90},
		{"id":"a711c33b4e5b49a3","x":-20,"y":920,"width":440,"height":90,"type":"text","text":"## Técnicas Modernas: Redes _Feedforward_, AdaIN e Além"},
		{"id":"c67477001e767625","x":380,"y":640,"width":880,"height":220,"type":"text","text":"**Contexto e Motivação**\n\n- O método de **Gatys et al. (2016)** revolucionou a geração de arte neural, mas era **lento e caro**, pois cada nova combinação de conteúdo e estilo exigia **otimização iterativa nos pixels**.\n- Isso tornava o processo inviável para aplicações práticas em tempo real, limitando seu uso a pesquisas e experimentos com alto poder computacional.\n- Para superar essa limitação, surgiram as **redes feedforward**, que deslocam o custo de otimização para uma **etapa de treinamento único**, permitindo aplicar o estilo **rapidamente em novas imagens** — quase instantaneamente."},
		{"id":"2bffa3ad00491f69","x":880,"y":920,"width":582,"height":73,"type":"text","text":"A partir do método original, o campo se dividiu em **três linhas principais**, cada uma equilibrando **velocidade, flexibilidade e generalização**:"},
		{"id":"e631db109b282913","x":1543,"y":750,"width":937,"height":170,"type":"text","text":"**a) Modelos de Estilo Único** (Redes Feedforward de Estilo Específico)\n- Treinam uma **rede feedforward** para aplicar **um estilo específico** a qualquer imagem de conteúdo.\n- Após o treinamento, a aplicação é **quase instantânea**, pois basta uma passada pela rede.\n- Limitação: cada rede só gera **um estilo** — é necessário treinar uma nova rede para cada novo estilo.\n- Exemplos: **Johnson et al. (2016); Ulyanov et al. (2017)**."},
		{"id":"32361d7270c9a720","x":1543,"y":940,"width":937,"height":240,"type":"text","text":"**b) Multiestilo em um Único Modelo**\n- Diferente dos modelos de estilo único, que exigem uma rede para cada estilo, o modelo multiestilo permite aplicar diversos estilos artísticos em uma única rede.\n- Introduzem **normalizações condicionadas**, permitindo que **um único Gerador** aplique **vários estilos**.\n- O estilo desejado é controlado alterando parâmetros de normalização (como a média e o desvio padrão das features).\n- Mais **flexíveis** que os modelos de estilo único, mas ainda **limitados a um conjunto finito de estilos** vistos durante o treinamento.\n- Exemplo: **Dumoulin; Shlens; Kudlur (2017)**."},
		{"id":"fa5218a0a5c8244b","x":1543,"y":1200,"width":937,"height":260,"type":"text","text":"**c) AdaIN (Adaptive Instance Normalization)**\n- O **AdaIN** foi um marco, pois possibilitou **transferência de estilo arbitrária**, ou seja, **qualquer combinação de conteúdo e estilo** pode ser feita **sem retreinar o modelo**.\n- Ele **ajusta dinamicamente as estatísticas** (média e variância) das ativações da imagem de conteúdo para igualá-las às da imagem de estilo.\n- Assim, a rede adapta instantaneamente o estilo e mantém a estrutura do conteúdo.\n- Resultado: **estilização rápida, flexível e generalizada**, aplicável em tempo real.\n- Exemplo: **Huang; Belongie (2017)**."},
		{"id":"a488db641f095650","x":2640,"y":720,"width":640,"height":402,"type":"text","text":"![[mult.png]]"},
		{"id":"4d36abc56e57441f","x":2640,"y":1180,"width":600,"height":500,"type":"text","text":"![[adain.png]]"},
		{"id":"98e0a545fc1a3f67","x":1543,"y":1520,"width":937,"height":300,"type":"text","text":"**Como Funciona**\n\n- O método se baseia na observação de que a **aparência estética de uma imagem** está codificada em **estatísticas simples** das ativações internas de uma CNN — principalmente **média** e **variância**.\n- O processo ocorre em três etapas principais:\n    1. **Codificador de estilo**: extrai as estatísticas (média e variância) da imagem de estilo.\n    2. **Codificador de conteúdo**: obtém as features estruturais da imagem base.\n    3. **Bloco AdaIN**: ajusta dinamicamente as ativações do conteúdo para que tenham as **mesmas estatísticas** do estilo, injetando sua estética no espaço latente.\n- Por fim, um **decodificador** reconstrói a imagem final, fundindo **estrutura (conteúdo)** e **aparência (estilo)** de forma suave e coerente."}
	],
	"edges":[
		{"id":"359e3428515649c7","fromNode":"9fcbe3b2272e3279","fromSide":"top","toNode":"9c97701ce2294a94","toSide":"left"},
		{"id":"fa0b966273b224b4","fromNode":"9fcbe3b2272e3279","fromSide":"right","toNode":"6f4f949fe721bfc7","toSide":"left"},
		{"id":"9d257d38c2f62c29","fromNode":"6f4f949fe721bfc7","fromSide":"right","toNode":"cfa9625eec49d937","toSide":"left"},
		{"id":"7cd1ef920a55e7bb","fromNode":"6f4f949fe721bfc7","fromSide":"right","toNode":"efba9e77dafedac4","toSide":"left"},
		{"id":"0225a3b10967bd08","fromNode":"6f4f949fe721bfc7","fromSide":"right","toNode":"066be457e5668626","toSide":"left"},
		{"id":"ac418f824d267a34","fromNode":"efba9e77dafedac4","fromSide":"right","toNode":"729d3196fde61a0e","toSide":"left"},
		{"id":"a5bebcdecb1b2900","fromNode":"efba9e77dafedac4","fromSide":"right","toNode":"9387e10b6f899443","toSide":"left"},
		{"id":"1dd8565c0215b923","fromNode":"9fcbe3b2272e3279","fromSide":"right","toNode":"a711c33b4e5b49a3","toSide":"left"},
		{"id":"5bd07125eccdd594","fromNode":"a711c33b4e5b49a3","fromSide":"top","toNode":"c67477001e767625","toSide":"left"},
		{"id":"ffad45b5ff98ff97","fromNode":"c67477001e767625","fromSide":"bottom","toNode":"2bffa3ad00491f69","toSide":"left"},
		{"id":"c2bbb4921a1bdd02","fromNode":"2bffa3ad00491f69","fromSide":"right","toNode":"e631db109b282913","toSide":"left"},
		{"id":"81ba9b6abf6c7256","fromNode":"2bffa3ad00491f69","fromSide":"right","toNode":"32361d7270c9a720","toSide":"left"},
		{"id":"474f896a59d76aac","fromNode":"2bffa3ad00491f69","fromSide":"right","toNode":"fa5218a0a5c8244b","toSide":"left"},
		{"id":"3cf6137ff67594a5","fromNode":"32361d7270c9a720","fromSide":"right","toNode":"a488db641f095650","toSide":"left"},
		{"id":"ee6227d8aeb98364","fromNode":"fa5218a0a5c8244b","fromSide":"right","toNode":"4d36abc56e57441f","toSide":"left"},
		{"id":"a00871e4d1571fb3","fromNode":"fa5218a0a5c8244b","fromSide":"bottom","toNode":"98e0a545fc1a3f67","toSide":"top"}
	]
}